{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = {\n",
    "    \"data_path\": \"../\",\n",
    "    \"model\": {\n",
    "        \"loss_smooth\": 1.0,\n",
    "        \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 0.0},\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"CosineAnnealingLR\",\n",
    "            \"params\": {\n",
    "                \"CosineAnnealingLR\": {\"T_max\": 50, \"eta_min\": 1e-06, \"last_epoch\": -1},\n",
    "                \"ReduceLROnPlateau\": {\n",
    "                    \"factor\": 0.316,\n",
    "                    \"mode\": \"min\",\n",
    "                    \"patience\": 3,\n",
    "                    \"verbose\": True,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"output_dir\": \"models\",\n",
    "    \"progress_bar_refresh_rate\": 500,\n",
    "    \"seed\": 42,\n",
    "    \"train_bs\": 64,\n",
    "    \"trainer\": {\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"max_epochs\": 50,\n",
    "        \"min_epochs\": 30,\n",
    "        \"accelerator\": \"mps\",\n",
    "        \"devices\": 1,\n",
    "    },\n",
    "    \"valid_bs\": 64,\n",
    "    \"workers\": 0,\n",
    "    \"device\": \"mps\",\n",
    "    \"folds\": {\n",
    "        \"n_splits\": 4,\n",
    "        \"random_state\": 42,\n",
    "        \"train_folds\": [0, 1, 2, 3]\n",
    "    }\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/single_turbine_data/train_reduced_unskewed_extra.csv')\n",
    "test = pd.read_csv('data/single_turbine_data/test_reduced_unskewed_extra.csv')\n",
    "\n",
    "label = ['1_Gear oil temperature (°C)']\n",
    "\n",
    "X_train = train.drop(label, axis=1)\n",
    "y_train = train[label]\n",
    "X_test = test.drop(label, axis=1)\n",
    "y_test = test[label]\n",
    "\n",
    "# convert to datetime\n",
    "X_train['# Date and time'] = pd.to_datetime(X_train['# Date and time'])\n",
    "X_test['# Date and time'] = pd.to_datetime(X_test['# Date and time'])\n",
    "\n",
    "# Setting the index\n",
    "X_train.set_index('# Date and time', inplace=True)\n",
    "X_test.set_index('# Date and time', inplace=True)\n",
    "\n",
    "original_cols = ['1_Wind direction (°)',\n",
    "       '1_Nacelle position (°)', '1_Power (kW)',\n",
    "       '1_Front bearing temperature (°C)', '1_Rear bearing temperature (°C)',\n",
    "       '1_Stator temperature 1 (°C)', '1_Nacelle ambient temperature (°C)',\n",
    "       '1_Nacelle temperature (°C)', '1_Transformer temperature (°C)',\n",
    "       '1_Generator bearing rear temperature (°C)',\n",
    "       '1_Generator bearing front temperature (°C)', '1_Temp. top box (°C)',\n",
    "       '1_Hub temperature (°C)', '1_Ambient temperature (converter) (°C)',\n",
    "       '1_Rotor bearing temp (°C)', '1_Transformer cell temperature (°C)', '1_Generator RPM (RPM)']\n",
    "extras = ['month_sin', 'month_cos', 'hour_sin', 'hour_cos', \n",
    "'curtailed', \n",
    "'offline',\n",
    "]\n",
    "leadsnlags = [\n",
    "       '1_Wind direction (°)_lead6', \n",
    "       '1_Nacelle position (°)_lead3',\n",
    "       '1_Power (kW)_lag6', \n",
    "       '1_Stator temperature 1 (°C)',\n",
    "       '1_Stator temperature 1 (°C)_lag1',\n",
    "       '1_Nacelle ambient temperature (°C)_lead6',\n",
    "       '1_Transformer temperature (°C)_lead6',\n",
    "       '1_Generator bearing rear temperature (°C)_lag1',\n",
    "       '1_Temp. top box (°C)_lag1', \n",
    "       '1_Hub temperature (°C)_lead6',\n",
    "       '1_Ambient temperature (converter) (°C)_lead6',\n",
    "       '1_Transformer cell temperature (°C)_lead6',\n",
    "       '1_Generator RPM (RPM)_lead6']\n",
    "\n",
    "cols = original_cols + extras + leadsnlags\n",
    "X_test = X_test[cols]\n",
    "X_train = X_train[cols]\n",
    "\n",
    "# scale the data\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = scaler.fit_transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "import lightning.pytorch as pl\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# Convert Pandas DataFrame to PyTorch Tensor\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def quantile_loss(preds, targets, alpha=0.05):\n",
    "    errors = targets - preds\n",
    "    \n",
    "    lower_quantile = torch.max((alpha - 1) * errors, alpha * errors).mean()\n",
    "    upper_quantile = torch.max(((1 - alpha) - 1) * errors, (1 - alpha) * errors).mean()\n",
    "    \n",
    "    return lower_quantile + upper_quantile\n",
    "\n",
    "\n",
    "\n",
    "# Define Neural Network\n",
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer1 = nn.Linear(input_dim, input_dim*2)\n",
    "        self.layer2 = nn.Linear(input_dim*2, input_dim*2)\n",
    "        self.layer3 = nn.Linear(input_dim*2, output_dim)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.relu = nn.LeakyReLU(0.3)\n",
    "        # self.dropout = nn.Dropout(p=0.2)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters())\n",
    "\n",
    "        if self.config['model'][\"scheduler\"][\"name\"] == \"CosineAnnealingLR\":\n",
    "            scheduler = CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                **self.config['model'][\"scheduler\"][\"params\"][self.config['model'][\"scheduler\"][\"name\"]],\n",
    "            )\n",
    "            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n",
    "        elif self.config['model'][\"scheduler\"][\"name\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                **self.config['model'][\"scheduler\"][\"params\"][self.config['model'][\"scheduler\"][\"name\"]],\n",
    "            )\n",
    "            lr_scheduler = {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        for param_group in self.trainer.optimizers[0].param_groups:\n",
    "            lr = param_group[\"lr\"]\n",
    "        self.log(\"lr\", lr, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        logs = {\"train_loss\": loss, \"lr\": lr}\n",
    "        return {\"loss\": loss, \"log\": logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Prepare the KFold cross-validation\n",
    "kf = KFold(n_splits=config[\"folds\"][\"n_splits\"])\n",
    "\n",
    "# Initialize results DataFrame\n",
    "overall_results = pd.DataFrame()\n",
    "\n",
    "# Main loop for K-Fold Cross-Validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print(f\"Training Fold {fold+1}\")\n",
    "    # Subset the data\n",
    "    X_train_fold = X_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    X_val_fold = X_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "\n",
    "    # Create DataLoader for this fold\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_fold), torch.FloatTensor(y_train_fold))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"train_bs\"], shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val_fold), torch.FloatTensor(y_val_fold))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"valid_bs\"], shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = LightningModule(X_train.shape[1], y_train.shape[1])\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=1)\n",
    "    progress_bar_callback = TQDMProgressBar(refresh_rate=config[\"progress_bar_refresh_rate\"])\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    verbose=1,\n",
    "    dirpath=config[\"output_dir\"],\n",
    "    filename=\"best_model_fold_\" + str(fold + 1),\n",
    ")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[early_stop_callback, progress_bar_callback, checkpoint_callback],\n",
    "        logger=logger,\n",
    "        **config[\"trainer\"]\n",
    "    )\n",
    "\n",
    "    # Train the model on this fold\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    y_pred_fold = model(torch.FloatTensor(X_val_fold)).detach().numpy()\n",
    "\n",
    "    # Inverse scaling for validation data and prediction\n",
    "    y_val_fold = scaler.inverse_transform(y_val_fold)\n",
    "    y_pred_fold = scaler.inverse_transform(y_pred_fold)\n",
    "\n",
    "    # Evaluate the model on this fold\n",
    "    rmse_fold = sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "    print(f'Validation RMSE for Fold {fold+1}: {rmse_fold}')\n",
    "\n",
    "    # Save results for this fold\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            'Model': f'Neural Network - Fold {fold+1}',\n",
    "            'Validation RMSE': rmse_fold,\n",
    "            'Iterations': trainer.global_step,\n",
    "            'Learning Rate': config['model']['optimizer_params']['lr'],\n",
    "            'Depth': [X_train.shape[1], 64, 64, y_train.shape[1]],\n",
    "            'Loss Function': 'MSE',\n",
    "            'Features': ', '.join(cols),\n",
    "        }\n",
    "    )\n",
    "    overall_results = pd.concat([overall_results, results_df])\n",
    "\n",
    "    # save model\n",
    "    # torch.save(model.state_dict(), f\"models/nn_fold{fold+1}.pth\")\n",
    "\n",
    "from pathlib import Path\n",
    "# Save results\n",
    "results_file = Path('results.csv')\n",
    "# Save overall results\n",
    "if results_file.exists():\n",
    "    existing_df = pd.read_csv(results_file)\n",
    "    overall_results = pd.concat([existing_df, overall_results])\n",
    "\n",
    "overall_results.to_csv(results_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# torch.save(model.state_dict(), 'nn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize variables\n",
    "combined_preds = []\n",
    "path = 'models'\n",
    "\n",
    "# unscale the data\n",
    "# if the mean of y_test is < 10 \n",
    "if y_test.mean() < 10:\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Iterate through all models in the folder\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('v2.ckpt'):\n",
    "        checkpoint_path = os.path.join(path, filename)\n",
    "        \n",
    "        state_dict = torch.load(checkpoint_path)\n",
    "        model = LightningModule(X_train.shape[1], y_train.shape[1])\n",
    "        model.load_state_dict(state_dict['state_dict'])\n",
    "        \n",
    "        # Switch to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test_tensor)\n",
    "        \n",
    "        # Process predictions\n",
    "        y_pred = y_pred.detach().numpy()\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "        \n",
    "        # Store predictions for later\n",
    "        combined_preds.append(y_pred)\n",
    "\n",
    "# Combine all predictions (mean ensemble, you can also try other techniques)\n",
    "# combined_preds = np.mean(np.array(combined_preds), axis=0)\n",
    "\n",
    "# Calculate the combined RMSE\n",
    "# combined_rmse = sqrt(mean_squared_error(y_test, combined_preds))\n",
    "# print(f'Combined Test RMSE: {combined_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test RMSE: 0.475 with weights (0.1, 0.0, 0.30000000000000004, 0.6000000000000001)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "# Generate weight combinations, summing to 1\n",
    "num_models = len(combined_preds)\n",
    "for weights in itertools.product(np.linspace(0, 1, 11), repeat=num_models):\n",
    "    if sum(weights) == 1:\n",
    "        \n",
    "        # Calculate the weighted average\n",
    "        combined_preds_weighted = np.average(np.array(combined_preds), axis=0, weights=weights)\n",
    "        \n",
    "        # Calculate RMSE for the weighted average\n",
    "        rmse_weighted = sqrt(mean_squared_error(y_test, combined_preds_weighted))\n",
    "        \n",
    "        # Update the best RMSE and corresponding weights\n",
    "        if rmse_weighted < best_rmse:\n",
    "            best_rmse = rmse_weighted\n",
    "            best_weights = weights\n",
    "\n",
    "# Show the best RMSE and weights\n",
    "print(f'Best Test RMSE: {best_rmse:.3f} with weights {best_weights}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hacakthon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
