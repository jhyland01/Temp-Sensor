{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv('data/single_turbine_data/train_reduced_unskewed.csv')\n",
    "test = pd.read_csv('data/single_turbine_data/test_reduced_unskewed.csv')\n",
    "\n",
    "label = ['1_Gear oil temperature (°C)']\n",
    "\n",
    "X_train = train.drop(label, axis=1)\n",
    "y_train = train[label]\n",
    "X_test = test.drop(label, axis=1)\n",
    "y_test = test[label]\n",
    "\n",
    "# convert to datetime\n",
    "X_train['# Date and time'] = pd.to_datetime(X_train['# Date and time'])\n",
    "X_test['# Date and time'] = pd.to_datetime(X_test['# Date and time'])\n",
    "# y_train['# Date and time'] = pd.to_datetime(y_train['# Date and time'])\n",
    "# y_test['# Date and time'] = pd.to_datetime(y_test['# Date and time'])\n",
    "\n",
    "# Setting the index\n",
    "X_train.set_index('# Date and time', inplace=True)\n",
    "X_test.set_index('# Date and time', inplace=True)\n",
    "# y_train.set_index('# Date and time', inplace=True)\n",
    "# y_test.set_index('# Date and time', inplace=True)\n",
    "\n",
    "original_cols = ['1_Wind direction (°)',\n",
    "       '1_Nacelle position (°)', '1_Power (kW)',\n",
    "       '1_Front bearing temperature (°C)', '1_Rear bearing temperature (°C)',\n",
    "       '1_Stator temperature 1 (°C)', '1_Nacelle ambient temperature (°C)',\n",
    "       '1_Nacelle temperature (°C)', '1_Transformer temperature (°C)',\n",
    "       '1_Generator bearing rear temperature (°C)',\n",
    "       '1_Generator bearing front temperature (°C)', '1_Temp. top box (°C)',\n",
    "       '1_Hub temperature (°C)', '1_Ambient temperature (converter) (°C)',\n",
    "       '1_Rotor bearing temp (°C)', '1_Transformer cell temperature (°C)', '1_Generator RPM (RPM)']\n",
    "extras = ['month_sin', 'month_cos', 'hour_sin', 'hour_cos', 'curtailed', 'offline']\n",
    "unskewed = [col for col in train.columns if col.endswith('unsk')]\n",
    "\n",
    "# Create a set for each list of features\n",
    "original_cols_set = set(original_cols)\n",
    "extras_set = set(extras)\n",
    "unskewed_set = set(unskewed)\n",
    "\n",
    "# Remove the '_unsk' suffix from the unskewed features\n",
    "unskewed_without_suffix = {feat.replace('unsk', '') for feat in unskewed_set}\n",
    "\n",
    "# Subtract the unskewed set (without suffix) from the original set\n",
    "# This will give you only the features in the original set that don't have an unskewed version\n",
    "original_cols_without_unskewed = original_cols_set - unskewed_without_suffix\n",
    "\n",
    "# Now create the final list of features\n",
    "# This contains all the extra features, the unskewed features, and the original features that don't have an unskewed version\n",
    "final_features = list(original_cols_without_unskewed | extras_set | unskewed_set)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = original_cols + extras\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]\n",
    "\n",
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=False).get_n_splits(X_train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.005, \n",
    "                                            random_state=1,\n",
    "                                            fit_intercept=True,\n",
    "                                            warm_start=True,\n",
    "                                            max_iter=10000,\n",
    "                                            ))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.005, \n",
    "                                                l1_ratio=.9, \n",
    "                                                random_state=3))\n",
    "\n",
    "KRR = KernelRidge(alpha=0.006, kernel='polynomial', degree=2, coef0=1)\n",
    "\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.01,\n",
    "                                   max_depth=12, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='squared_error', \n",
    "                                   random_state =5)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.01, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=10000,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.01, n_estimators=10000,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "model_cat = CatBoostRegressor(\n",
    "    iterations=10000,\n",
    "    learning_rate=0.01,\n",
    "    depth=7,\n",
    "    loss_function='RMSE',\n",
    "    random_seed=42,\n",
    "    verbose=100  # Output every 100th iteration\n",
    ")\n",
    "\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)  \n",
    "    \n",
    "averaged_models = AveragingModels(models = (\n",
    "                                            # ENet, \n",
    "                                            # GBoost, \n",
    "                                            model_xgb,\n",
    "                                            model_cat,\n",
    "                                            model_lgb,\n",
    "                                            # KRR, \n",
    "                                            # lasso,\n",
    "                                            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:32:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1685694848790/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "0:\tlearn: 5.5195085\ttotal: 5.58ms\tremaining: 55.8s\n",
      "100:\tlearn: 2.5508464\ttotal: 541ms\tremaining: 53s\n",
      "200:\tlearn: 1.4758835\ttotal: 1.08s\tremaining: 52.5s\n",
      "300:\tlearn: 1.0858027\ttotal: 1.61s\tremaining: 52s\n",
      "400:\tlearn: 0.9375821\ttotal: 2.13s\tremaining: 51.1s\n",
      "500:\tlearn: 0.8553834\ttotal: 2.65s\tremaining: 50.3s\n",
      "600:\tlearn: 0.7975573\ttotal: 3.19s\tremaining: 49.8s\n",
      "700:\tlearn: 0.7537241\ttotal: 3.71s\tremaining: 49.3s\n",
      "800:\tlearn: 0.7182871\ttotal: 4.24s\tremaining: 48.7s\n",
      "900:\tlearn: 0.6883085\ttotal: 4.78s\tremaining: 48.2s\n",
      "1000:\tlearn: 0.6642717\ttotal: 5.3s\tremaining: 47.7s\n",
      "1100:\tlearn: 0.6437271\ttotal: 5.83s\tremaining: 47.1s\n",
      "1200:\tlearn: 0.6262437\ttotal: 6.35s\tremaining: 46.5s\n",
      "1300:\tlearn: 0.6107250\ttotal: 6.88s\tremaining: 46s\n",
      "1400:\tlearn: 0.5965876\ttotal: 7.4s\tremaining: 45.4s\n",
      "1500:\tlearn: 0.5840892\ttotal: 7.92s\tremaining: 44.9s\n",
      "1600:\tlearn: 0.5728052\ttotal: 8.45s\tremaining: 44.3s\n",
      "1700:\tlearn: 0.5625956\ttotal: 8.97s\tremaining: 43.8s\n",
      "1800:\tlearn: 0.5535807\ttotal: 9.5s\tremaining: 43.3s\n",
      "1900:\tlearn: 0.5450790\ttotal: 10s\tremaining: 42.7s\n",
      "2000:\tlearn: 0.5371107\ttotal: 10.5s\tremaining: 42.2s\n",
      "2100:\tlearn: 0.5300177\ttotal: 11.1s\tremaining: 41.6s\n",
      "2200:\tlearn: 0.5232027\ttotal: 11.6s\tremaining: 41s\n",
      "2300:\tlearn: 0.5168239\ttotal: 12.1s\tremaining: 40.5s\n",
      "2400:\tlearn: 0.5109100\ttotal: 12.6s\tremaining: 40s\n",
      "2500:\tlearn: 0.5054125\ttotal: 13.2s\tremaining: 39.5s\n",
      "2600:\tlearn: 0.4999030\ttotal: 13.7s\tremaining: 39s\n",
      "2700:\tlearn: 0.4950388\ttotal: 14.2s\tremaining: 38.5s\n",
      "2800:\tlearn: 0.4902054\ttotal: 14.8s\tremaining: 38s\n",
      "2900:\tlearn: 0.4855042\ttotal: 15.3s\tremaining: 37.5s\n",
      "3000:\tlearn: 0.4811994\ttotal: 15.8s\tremaining: 37s\n",
      "3100:\tlearn: 0.4770510\ttotal: 16.4s\tremaining: 36.4s\n",
      "3200:\tlearn: 0.4730193\ttotal: 16.9s\tremaining: 35.9s\n",
      "3300:\tlearn: 0.4692123\ttotal: 17.4s\tremaining: 35.3s\n",
      "3400:\tlearn: 0.4656456\ttotal: 17.9s\tremaining: 34.8s\n",
      "3500:\tlearn: 0.4621448\ttotal: 18.5s\tremaining: 34.3s\n",
      "3600:\tlearn: 0.4588330\ttotal: 19s\tremaining: 33.7s\n",
      "3700:\tlearn: 0.4556663\ttotal: 19.5s\tremaining: 33.2s\n",
      "3800:\tlearn: 0.4524026\ttotal: 20s\tremaining: 32.7s\n",
      "3900:\tlearn: 0.4494175\ttotal: 20.6s\tremaining: 32.2s\n",
      "4000:\tlearn: 0.4465507\ttotal: 21.1s\tremaining: 31.7s\n",
      "4100:\tlearn: 0.4436839\ttotal: 21.6s\tremaining: 31.1s\n",
      "4200:\tlearn: 0.4409809\ttotal: 22.2s\tremaining: 30.6s\n",
      "4300:\tlearn: 0.4383465\ttotal: 22.7s\tremaining: 30.1s\n",
      "4400:\tlearn: 0.4357632\ttotal: 23.2s\tremaining: 29.5s\n",
      "4500:\tlearn: 0.4332026\ttotal: 23.8s\tremaining: 29s\n",
      "4600:\tlearn: 0.4308136\ttotal: 24.3s\tremaining: 28.5s\n",
      "4700:\tlearn: 0.4284020\ttotal: 24.8s\tremaining: 28s\n",
      "4800:\tlearn: 0.4260491\ttotal: 25.4s\tremaining: 27.5s\n",
      "4900:\tlearn: 0.4238513\ttotal: 25.9s\tremaining: 27s\n",
      "5000:\tlearn: 0.4216426\ttotal: 26.4s\tremaining: 26.4s\n",
      "5100:\tlearn: 0.4194356\ttotal: 27s\tremaining: 25.9s\n",
      "5200:\tlearn: 0.4172474\ttotal: 27.5s\tremaining: 25.4s\n",
      "5300:\tlearn: 0.4151668\ttotal: 28.1s\tremaining: 24.9s\n",
      "5400:\tlearn: 0.4131348\ttotal: 28.6s\tremaining: 24.3s\n",
      "5500:\tlearn: 0.4111137\ttotal: 29.1s\tremaining: 23.8s\n",
      "5600:\tlearn: 0.4091233\ttotal: 29.7s\tremaining: 23.3s\n",
      "5700:\tlearn: 0.4072425\ttotal: 30.2s\tremaining: 22.8s\n",
      "5800:\tlearn: 0.4053290\ttotal: 30.7s\tremaining: 22.3s\n",
      "5900:\tlearn: 0.4035133\ttotal: 31.3s\tremaining: 21.7s\n",
      "6000:\tlearn: 0.4017364\ttotal: 31.8s\tremaining: 21.2s\n",
      "6100:\tlearn: 0.3999386\ttotal: 32.4s\tremaining: 20.7s\n",
      "6200:\tlearn: 0.3982779\ttotal: 32.9s\tremaining: 20.2s\n",
      "6300:\tlearn: 0.3965723\ttotal: 33.4s\tremaining: 19.6s\n",
      "6400:\tlearn: 0.3948698\ttotal: 34s\tremaining: 19.1s\n",
      "6500:\tlearn: 0.3931940\ttotal: 34.5s\tremaining: 18.6s\n",
      "6600:\tlearn: 0.3916022\ttotal: 35s\tremaining: 18s\n",
      "6700:\tlearn: 0.3900125\ttotal: 35.6s\tremaining: 17.5s\n",
      "6800:\tlearn: 0.3884532\ttotal: 36.1s\tremaining: 17s\n",
      "6900:\tlearn: 0.3869843\ttotal: 36.6s\tremaining: 16.5s\n",
      "7000:\tlearn: 0.3854951\ttotal: 37.2s\tremaining: 15.9s\n",
      "7100:\tlearn: 0.3841305\ttotal: 37.7s\tremaining: 15.4s\n",
      "7200:\tlearn: 0.3827164\ttotal: 38.2s\tremaining: 14.9s\n",
      "7300:\tlearn: 0.3813291\ttotal: 38.7s\tremaining: 14.3s\n",
      "7400:\tlearn: 0.3799896\ttotal: 39.3s\tremaining: 13.8s\n",
      "7500:\tlearn: 0.3786191\ttotal: 39.8s\tremaining: 13.3s\n",
      "7600:\tlearn: 0.3772701\ttotal: 40.3s\tremaining: 12.7s\n",
      "7700:\tlearn: 0.3758934\ttotal: 40.9s\tremaining: 12.2s\n",
      "7800:\tlearn: 0.3745594\ttotal: 41.4s\tremaining: 11.7s\n",
      "7900:\tlearn: 0.3733292\ttotal: 42s\tremaining: 11.2s\n",
      "8000:\tlearn: 0.3720154\ttotal: 42.5s\tremaining: 10.6s\n",
      "8100:\tlearn: 0.3707800\ttotal: 43s\tremaining: 10.1s\n",
      "8200:\tlearn: 0.3695432\ttotal: 43.6s\tremaining: 9.56s\n",
      "8300:\tlearn: 0.3683020\ttotal: 44.1s\tremaining: 9.03s\n",
      "8400:\tlearn: 0.3671498\ttotal: 44.7s\tremaining: 8.5s\n",
      "8500:\tlearn: 0.3660013\ttotal: 45.2s\tremaining: 7.97s\n",
      "8600:\tlearn: 0.3648471\ttotal: 45.7s\tremaining: 7.44s\n",
      "8700:\tlearn: 0.3637124\ttotal: 46.3s\tremaining: 6.91s\n",
      "8800:\tlearn: 0.3625943\ttotal: 46.8s\tremaining: 6.38s\n",
      "8900:\tlearn: 0.3614609\ttotal: 47.3s\tremaining: 5.84s\n",
      "9000:\tlearn: 0.3604103\ttotal: 47.9s\tremaining: 5.31s\n",
      "9100:\tlearn: 0.3592826\ttotal: 48.4s\tremaining: 4.78s\n",
      "9200:\tlearn: 0.3581578\ttotal: 49s\tremaining: 4.25s\n",
      "9300:\tlearn: 0.3570916\ttotal: 49.5s\tremaining: 3.72s\n",
      "9400:\tlearn: 0.3560681\ttotal: 50s\tremaining: 3.19s\n",
      "9500:\tlearn: 0.3550612\ttotal: 50.6s\tremaining: 2.66s\n",
      "9600:\tlearn: 0.3540444\ttotal: 51.1s\tremaining: 2.12s\n",
      "9700:\tlearn: 0.3530869\ttotal: 51.7s\tremaining: 1.59s\n",
      "9800:\tlearn: 0.3521306\ttotal: 52.2s\tremaining: 1.06s\n",
      "9900:\tlearn: 0.3511415\ttotal: 52.7s\tremaining: 527ms\n",
      "9999:\tlearn: 0.3501985\ttotal: 53.3s\tremaining: 0us\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnny/anaconda3/envs/hacakthon/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 0.5822915161106266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "averaged_models.fit(X_train.values, y_train)\n",
    "\n",
    "y_pred = averaged_models.predict(X_test.values)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"RMSE on test set: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(model_cat)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet score: 1.0046 (0.0768)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = rmsle_cv(KRR)\n",
    "# print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:27:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1685694848790/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[17:28:07] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1685694848790/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[17:28:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1685694848790/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[17:29:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1685694848790/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[17:30:33] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1685694848790/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "Xgboost score: 0.6744 (0.0715)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnny/anaconda3/envs/hacakthon/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/johnny/anaconda3/envs/hacakthon/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnny/anaconda3/envs/hacakthon/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnny/anaconda3/envs/hacakthon/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnny/anaconda3/envs/hacakthon/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "LGBM score: 0.9437 (0.1279)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
